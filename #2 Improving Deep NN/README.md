# Course 2: Improving Deep Neural Networks

### About this Course:
This course will teach you the "magic" of getting deep learning to work well. Rather than the deep learning process being a black box, you will understand what drives performance, and be able to more systematically get good results. You will also learn TensorFlow. After 3 weeks, you will:
* Understand industry best-practices for building deep learning applications. 
* Be able to effectively use the common neural network "tricks", including initialization, L2 and dropout regularization, batch normalization, gradient checking, 
* Be able to implement and apply a variety of optimization algorithms, such as mini-batch gradient descent, Momentum, RMSprop and Adam, and check for their convergence. 
* Understand new best-practices for the deep learning era of how to set up train/dev/test sets and analyze bias/variance
* Be able to implement a neural network in TensorFlow.

### Weeks:
* Week 1: Practical Aspect of Deep Learning
* Week 2: Optimization Algorithms
* Week 3: Hyperparameter tuning, Batch Normalization and Programming Frameworks

### File Directory:
* *DNN_Initialization.ipynb*: comparison of various initialization methods for DNN, including zero initialization, random initialization, and He initialization
* *DNN_Regularization.ipynb*: comparison of 3-layer DNN without regularization, with L2 regularization, and with dropout regularization
* *DNN_GradientChecking.ipynb*: implementation of 1-dimentional and N-dimentional gradient checking
* *DNN_Optimization.ipynb*: comparison of different optimization algorithms for DNN, including gradient descent, mini-batch gradient descent, momentum, and adaptive moment estimation (Adam)
* *DNN_TensorFlow.ipynb*: implementation of a multi-class classification DNN with the Adam optimization algorithm using the TensorFlow library
